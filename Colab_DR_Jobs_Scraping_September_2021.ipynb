{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Colab DR Jobs Scraping September 2021.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fagc/ScrappingData/blob/master/Colab_DR_Jobs_Scraping_September_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWMuYlEJ9bZs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "#####LinkedIn\n",
        "\n",
        "rawdata = pd.read_csv(r\"C:\\Users\\Gav\\Anaconda3\\Projects\\Jobs_scraping\\linkedin-jobs-2021-08_02-predicted.csv\")\n",
        "data = rawdata[rawdata['description'].notna()]\n",
        "\n",
        "###LinkedIn cleaning and keyword fitlering, date filtering\n",
        "\n",
        "unique = data.duplicated(subset=['description'])\n",
        "data['duplicated'] = unique\n",
        "\n",
        "##job description scraping\n",
        "targets = ['secur', 'risk manag', 'cyber', 'Risk Manag', 'SOC', 'Secur', 'Cyber', 'Penetration', 'incident response', 'Threat Intelligence', 'Vulnerability', 'Detection', 'Incident Response', 'Threat']\n",
        "keyword = data.description.apply(lambda sentence: any(word in sentence for word in targets))\n",
        "data['keyword_scraping'] = keyword\n",
        "\n",
        "##job title scraping\n",
        "title_targets = ['Retire', 'People', 'Benefit', 'Motor', 'Sourc', 'Area', 'Marine', 'hand', 'Rating', 'man', 'Trade', 'Propos', 'Learn', 'Medi', 'Operator', 'Pric', 'Operating', 'Librar', 'Regis', 'Know', 'Case', 'Appeal', 'Territory', 'House', 'Construct' ,'Elect', 'Fire ', 'Robot', 'Kni', 'Bagel', 'Custod', 'Sneak', 'Image', 'Service', 'Value', 'Real', 'Sell', 'Veg', 'Energy', 'Wine', 'ASSOC', 'Bake', 'Drive', 'Rural', 'Tast', 'Dish', 'Janit', 'Carpe', 'Pain', 'Investor', 'Port', 'Target', 'Barista', 'Chef', 'Cashier', 'Hospital', 'Cook', 'Stock', 'Food', 'Mile', 'Silicon', 'Neuro', 'Fund', 'Home', 'Credit', 'HR', 'Banking', 'Lube', 'Payroll', 'Cabin', 'SALES', 'Wealth', 'CLERK', 'Care', 'Warehouse', 'Valet', 'Parking', 'Merch', 'Property', 'Resident', 'Banker', 'Store', 'Supplier', 'Assets', 'Data Sci', 'Land', 'Restaurant', 'Pharma', 'Clerk', 'Attend', 'Pediatric', 'Valet', 'Store', 'Concierge', 'Workplace', 'Sale', 'Asset', 'Alcohol', 'Front', 'Loss Prevention', 'Guest', 'Buyer', 'Distrib', 'Clinic', 'Key', 'Heart', 'Material', 'Ling', 'Oral', 'Quality', 'Maint', 'Strategy', 'Field', 'Water', 'Pack', 'bound', 'Product', 'Electr', 'Counsel', 'Chem', 'Lab', 'Therapy', 'Logistics', 'Reward', 'Nurse', 'Paralegal' 'Hous', 'Media', 'Talent', 'Accounting', 'Geo', 'Loan', 'Equity', 'Asset Protection', 'Demand', 'Art', 'Video', 'Consumer', 'Dairy', 'Financ', 'Manufacturing', 'Safety', 'Health', 'Design', 'Repair', 'Software Dev', 'Software Engineer', 'Full Stack', 'Market' ,'Plant', 'Sales', 'Actuar', 'Claim', 'Legal', 'Human Resources', 'Client', 'Customer']\n",
        "title_keyword = data.job_title.apply(lambda sentence: any(word in sentence for word in title_targets))\n",
        "data['title_keyword_scraping'] = title_keyword\n",
        "\n",
        "\n",
        "#date filtering\n",
        "start = datetime.date(2021, 6, 1)\n",
        "\n",
        "def two_months_date(df):\n",
        "        \n",
        "    if (df['posted_date'] > start):\n",
        "        return 'True'\n",
        "    elif (df['posted_date'] < start):\n",
        "        return 'False'\n",
        "    else:\n",
        "        return 'True'\n",
        "    \n",
        "data[['posted_date']].replace('(/)','-',regex=True, inplace = True)\n",
        "data['posted_date'] = pd.to_datetime(data['posted_date']).dt.date\n",
        "data['date_criteria'] = data.apply(two_months_date, axis = 1)\n",
        "data['posted_date'] = data[['posted_date']].astype(str)\n",
        "\n",
        "#####INDEED\n",
        "\n",
        "rawdata_indeed = pd.read_csv(r\"C:\\Users\\Gav\\Anaconda3\\Projects\\Jobs_scraping\\indeed-jobs-2021-08-predicted.csv\")\n",
        "data_indeed = rawdata_indeed[rawdata_indeed['description'].notna()]\n",
        "data_indeed['seniority_level'] = 'Not Applicable'\n",
        "\n",
        "###INDEED cleaning and keyword fitlering\n",
        "\n",
        "unique = data_indeed.duplicated(subset=['description'])\n",
        "data_indeed['duplicated'] = unique\n",
        "\n",
        "##job description scraping\n",
        "keyword = data_indeed.description.apply(lambda sentence: any(word in sentence for word in targets))\n",
        "data_indeed['keyword_scraping'] = keyword\n",
        "\n",
        "##job title scraping\n",
        "title_keyword_indeed = data_indeed.job_title.apply(lambda sentence: any(word in sentence for word in title_targets))\n",
        "data_indeed['title_keyword_scraping'] = title_keyword_indeed\n",
        "\n",
        "#date filtering\n",
        "\n",
        "data_indeed[['posted_date']].replace('(/)','-',regex=True, inplace = True)\n",
        "data_indeed['posted_date'] = pd.to_datetime(data_indeed['posted_date']).dt.date\n",
        "data_indeed['date_criteria'] = data_indeed.apply(two_months_date, axis = 1)\n",
        "data_indeed['posted_date'] = data_indeed[['posted_date']].astype(str)\n",
        "\n",
        "\n",
        "##COMBINED\n",
        "\n",
        "data_all = data.append(data_indeed, sort=False)\n",
        "\n",
        "###reclass Cyber=1\n",
        "check_cyber = data_all.job_title.apply(lambda sentence: any(word in sentence for word in targets))\n",
        "data_all['without_doubt_cyber'] = check_cyber\n",
        "\n",
        "def check_cyber_f(df):\n",
        "\n",
        "    if (df['Cyber=1'] == 0 and df['without_doubt_cyber'] == True):\n",
        "        return 1\n",
        "    else:\n",
        "        return df['Cyber=1']\n",
        "    \n",
        "data_all['Cyber=1'] = data_all.apply(check_cyber_f, axis = 1)\n",
        "\n",
        "###FILTERED CSV\n",
        "\n",
        "data_filtered = data_all[(data_all['duplicated'] == False) & (data_all['Cyber=1'] == 1) & (data_all['keyword_scraping'] == True) & (data_all['title_keyword_scraping'] == False) & (data_all['date_criteria'] == 'True')]\n",
        "\n",
        "#'threat intelligence'  'incident response' \n",
        "#\n",
        "\n",
        "###TURN SENIORITY LEVEL INTO JR/MID/SR BREAKDOWN\n",
        "\n",
        "def seniority_df(df):\n",
        "\n",
        "    if (df['seniority_level'] == 'Internship' or df['seniority_level'] == 'Entry level'):\n",
        "        return 'Jr'\n",
        "    elif (df['seniority_level'] == 'Mid-Senior level' or df['seniority_level'] == 'Associate'):\n",
        "        return 'Mid'\n",
        "    elif (df['seniority_level'] == 'Director' or df['seniority_level'] == 'Executive'):\n",
        "        return 'Sr'\n",
        "    elif (df['seniority_level'] == 'Not Applicable' or df['seniority_level'] == ''):\n",
        "        return 'Not Applicable'\n",
        "\n",
        "data_filtered_2 = data_filtered\n",
        "data_filtered['seniority'] = data_filtered.apply(seniority_df, axis = 1)\n",
        "\n",
        "#not applicable classification with job description years of experience#####\n",
        "import re\n",
        "import ast\n",
        "\n",
        "data_filtered['yrs'] = data_filtered['description'].str.findall(r\"([\\d. +-/]+|[\\(\\d\\). +-/]+|[\\w\\+\\b]+)\\s*years of\")\n",
        "raw = data_filtered['yrs']\n",
        "\n",
        "\n",
        "data_filtered = data_filtered[~data_filtered.index.duplicated(keep='first')]\n",
        "\n",
        "data_filtered['yrs'] = data_filtered['yrs'].astype(str)\\\n",
        "         .str.replace(\".\", ' ')\\\n",
        ".str.replace(\"(\", '')\\\n",
        ".str.replace(\")\", '')\\\n",
        ".str.replace(\"+\", ' ')\\\n",
        ".str.replace(\"œ\", ' ')\\\n",
        ".str.replace(\"(\\d-)\", ' ',regex=True)\\\n",
        ".str.replace(\"(\\s-\\s\\d+)\", '',regex=True)\\\n",
        ".str.replace(\"(-)\", '',regex=True)\\\n",
        ".str.replace(\"(,1\\d|,10|ten|Ten)\", '10',regex=True)\\\n",
        ".str.replace(\"(,5|five|œ5|Five)\", '5',regex=True)\\\n",
        ".str.replace(\"eighteen\", '18',regex=True)\\\n",
        ".str.replace(\"(fifteen|,15)\", '15',regex=True)\\\n",
        ".str.replace(\"(Twelve|twelve)\", '12',regex=True)\\\n",
        ".str.replace(\"(,9|nine)\", '9',regex=True)\\\n",
        ".str.replace(\"(,3|three|Three)\", '3',regex=True)\\\n",
        ".str.replace(\"(,8|/8|eight|Eight|œ8)\", '8',regex=True)\\\n",
        ".str.replace(\"(,4|/4|four|Four)\", '4',regex=True)\\\n",
        ".str.replace(\"(,2|/2|two|Two|œ2)\", '2',regex=True)\\\n",
        ".str.replace(\"(,7|/7|seven)\", '7',regex=True)\\\n",
        ".str.replace(\"(,6|six)\", '6',regex=True)\\\n",
        ".str.replace(\",1\", '1',regex=True)\\\n",
        ".str.replace(\"[a-zA-Z+]\", '',regex=True)\\\n",
        ".str.replace(\"(',', | ','|/|,0|Â)\", '')\\\n",
        ".str.replace(\" \", '').apply(ast.literal_eval)\n",
        "\n",
        "data_filtered['yrs'] = data_filtered['yrs'].astype(str)\\\n",
        "         .str.replace(\".\", ' ')\\\n",
        ".str.replace(\"+\", ' ')\\\n",
        ".str.replace(\"(\\d-)\", '',regex=True)\\\n",
        ".str.replace(\"(-)\", '',regex=True)\\\n",
        ".str.replace(\"(,1\\d|,10)\", '10',regex=True)\\\n",
        ".str.replace(\",5\", '5',regex=True)\\\n",
        ".str.replace(\",9\", '9',regex=True)\\\n",
        ".str.replace(\",3\", '3',regex=True)\\\n",
        ".str.replace(\"(,8|/8)\", '8',regex=True)\\\n",
        ".str.replace(\"(,4|/4)\", '4',regex=True)\\\n",
        ".str.replace(\"(,2|/2)\", '2',regex=True)\\\n",
        ".str.replace(\"(,7|/7)\", '7',regex=True)\\\n",
        ".str.replace(\",6\", '6',regex=True)\\\n",
        ".str.replace(\",1\", '1',regex=True)\\\n",
        ".str.replace(\"(',', | ','|/|,0)\", '')\\\n",
        ".str.replace(\",,\", ',',regex=True)\\\n",
        ".str.replace(\" \", '').apply(ast.literal_eval)\n",
        "\n",
        "yrs_exp = data_filtered['yrs']\n",
        "\n",
        "data_filtered['yrs_exp'] = yrs_exp\n",
        "\n",
        "data_filtered['yrs_exp2'] = np.where(data_filtered['yrs_exp'].astype(str)\\\n",
        "         .str.contains(\"\\d\"), data_filtered['yrs_exp'], '0')\n",
        "data_filtered['yrs_exp2']\n",
        "\n",
        "seniority_data = data_filtered[(data_filtered['yrs_exp2'] != '0')]\n",
        "avgtest = seniority_data['yrs_exp2']\n",
        "avgtest2 = [' '.join(i).split() for i in avgtest]\n",
        "converted = [[int(num) for num in sub] for sub in avgtest2]\n",
        "means = [np.mean([el for el in sublist if el > 0] or 0) for sublist in converted]\n",
        "seniority_data['means'] = means\n",
        "seniority_data['means'] = seniority_data['means'].astype('int64')\n",
        "\n",
        "def not_applicable_seniority(df):\n",
        "        \n",
        "    if (df['means'] <= 3):\n",
        "        return 'Jr'\n",
        "    elif (df['means'] > 3 and df['means'] <= 10):\n",
        "        return 'Mid'\n",
        "    elif (df['means'] > 10 and df['means'] <= 25):\n",
        "        return 'Sr'\n",
        "    else:\n",
        "        return 'Not Applicable'\n",
        "    \n",
        "seniority_data['means_class'] = seniority_data.apply(not_applicable_seniority, axis = 1)\n",
        "\n",
        "def not_applicable_seniority_reclass(df):\n",
        "        \n",
        "    if (df['seniority'] == 'Not Applicable'):\n",
        "        return df['means_class']\n",
        "    else:\n",
        "        return df['seniority']\n",
        "    \n",
        "seniority_data['seniority_v2'] = seniority_data.apply(not_applicable_seniority_reclass, axis = 1)\n",
        "\n",
        "merged_table = data_filtered.merge(seniority_data[['seniority_v2']], how='left', left_index=True, right_index=True)\n",
        "merged_table['seniority_v2']= merged_table[['seniority_v2']].fillna(value=0)\n",
        "#merged_table\n",
        "\n",
        "def not_applicable_seniority_merge(df):\n",
        "        \n",
        "    if (df['seniority_v2'] == 0):\n",
        "        return df['seniority']\n",
        "    else:\n",
        "        return df['seniority_v2']\n",
        "\n",
        "merged_table['seniority_v3'] = merged_table.apply(not_applicable_seniority_merge, axis = 1)\n",
        "data_filtered['seniority_v3'] = merged_table['seniority_v3']\n",
        "\n",
        "#end of seniority classification#####\n",
        "\n",
        "jr_keywords = ['Intern', 'junior', 'Junior', 'jr', 'Jr', 'Entry']\n",
        "mid_keywords = ['Head', 'Pro', 'Specialist', 'Expert', 'Manager', 'Consultant', 'Lead', 'Partner', 'Senior', 'Sr', 'II', 'Advisor', 'Principal', 'Associate', 'Supervisor']\n",
        "sr_keywords = ['Director', 'Officer', 'Chief', 'Executive', 'BISO', 'CISO']\n",
        "\n",
        "title_keyword_jr = data_filtered.job_title.apply(lambda sentence: any(word in sentence for word in jr_keywords))\n",
        "data_filtered['jr_title_v4'] = title_keyword_jr\n",
        "\n",
        "title_keyword_mid = data_filtered.job_title.apply(lambda sentence: any(word in sentence for word in mid_keywords))\n",
        "data_filtered['mid_title_v4'] = title_keyword_mid\n",
        "\n",
        "title_keyword_sr = data_filtered.job_title.apply(lambda sentence: any(word in sentence for word in sr_keywords))\n",
        "data_filtered['sr_title_v4'] = title_keyword_sr\n",
        "\n",
        "def not_applicable_seniority_title(df):\n",
        "        \n",
        "    if (df['seniority_v3'] == 'Not Applicable' and df['jr_title_v4'] == True):\n",
        "        return 'Jr'\n",
        "    if (df['seniority_v3'] == 'Not Applicable' and df['mid_title_v4'] == True):\n",
        "        return 'Mid'\n",
        "    if (df['seniority_v3'] == 'Not Applicable' and df['sr_title_v4'] == True):\n",
        "        return 'Sr'\n",
        "    else:\n",
        "        return df['seniority_v3']\n",
        "\n",
        "data_filtered['seniority_v4'] = data_filtered.apply(not_applicable_seniority_title, axis = 1)\n",
        "\n",
        "\n",
        "######Build table with each CID having one row\n",
        "summary_table = data_filtered['cid'].drop_duplicates()\n",
        "summary_table_df = pd.DataFrame(summary_table)\n",
        "\n",
        "######calculate number  of jobs per cid\n",
        "\n",
        "df2=data_filtered.set_index('cid')\n",
        "g = df2.groupby(['cid']).sum()\n",
        "\n",
        "data_filtered_3 = pd.merge(summary_table_df, g, on=\"cid\", how=\"inner\")\n",
        "data_filtered_3.drop(columns =['applicant_count', 'duplicate', 'duplicated'])\n",
        "\n",
        "\n",
        "#####ASSIGN seniority VALUES TO CIDs\n",
        "\n",
        "jr = data_filtered.groupby(['cid', 'seniority_v4'])[['seniority_v4']].count()\n",
        "\n",
        "jr3 = jr.unstack(level='seniority_v4', fill_value=0)\n",
        "jr4 = jr3['seniority_v4']\n",
        "data_filtered_4 = pd.merge(data_filtered_3, jr4, on=\"cid\", how=\"left\")\n",
        "\n",
        "\n",
        "\n",
        "###remove unneeded columns\n",
        "data_filtered_5 = data_filtered_4.drop(columns =['applicant_count', 'Cyber=1', 'jr_title_v4', 'mid_title_v4', 'sr_title_v4', 'duplicate', 'duplicated', 'title_keyword_scraping'])\n",
        "data_final = data_filtered_5.rename(columns={\"keyword_scraping\": \"total_open_roles\"})\n",
        "\n",
        "### not applicable buckets\n",
        "data_final['Jr'] = data_final['Jr'] + .35 * data_final['Not Applicable']\n",
        "data_final['Mid'] = data_final['Mid'] + .60 * data_final['Not Applicable']\n",
        "data_final['Sr'] = data_final['Sr'] + .05 * data_final['Not Applicable']\n",
        "data_final['Not Applicable'] = 0\n",
        "\n",
        "data_finalv2 = data_final.round(decimals=0)\n",
        "data_finalv2['total_open_roles'] = data_finalv2['Jr'] + data_finalv2['Mid'] + data_finalv2['Sr']\n",
        "\n",
        "######CALCULATE HC OPEX NUMBERS\n",
        "\n",
        "data_finalv2['total HC OpEx'] = ((65000 * data_finalv2['Jr'] + 145000 * data_finalv2['Mid'] + 287500 * data_finalv2['Sr'])/12) * 1.35\n",
        "\n",
        "\n",
        "data_finalv3 = data_finalv2.fillna(0)\n",
        "data_finalv3"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}